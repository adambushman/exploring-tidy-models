d---
title: "Exploring {tidymodels}"
author: "Adam Bushman"
format: html
from: markdown+emoji
---

## Introduction

Throughout my BS in Data Science and Statistics, I was introduced to various machine learning algorithms and how to deploy them in R and Python. Within R, the packages, functions, and approach always felt jumbled and difficult to keep straight. Within Python, {sklearn} was a far more natural and friendly suite for data science.

The {tidymodels} suite of packages is the answer and conteporary of {sklearn} in the R ecosystem. My goal is to experiment with the various packages, learn the consistent approach, and resume data science projects and learning thereafter.

The below code and notes are an effort to document my learning and thoughts as a newcomer to the {tidymodels} suite but with a theoretical and practical background to data modeling.

## Setup

Thanks to the Tidyverse, all the packages needed for most data modeling in R are under the {tidymodels} umbrella. Upon installing and loading the library, we'll gain access to all the functionality of the packages therein.

Let's get started by setting up our environment with the libraries.

```{r}
#| warning: false
# Install the package if you haven't yet done so
# install.packages('tidymodels')

# Load the package for use
library('tidymodels')
```

With the library ready to go, we'll start exploring each package in turn to get a feel for their purpose and the functions they contain.

## Exploring each package

{tidymodels} features eight core packages used for modeling in the Tidyverse.

### A quick overview

##### [{rsample}](https://rsample.tidymodels.org/)

- Helps with sampling our data
- We can use the package to set up training and testing sets
    - "Training sets" are used to train our model how to perform well; this is usually 70-80% of our source data
    - "Testing sets" are used to test our model and evaluate its performance; this is the other 20-30% of our source data
- We can also use the package to execute cross-validation of different types
    - Bootstraps
    - V-fold
    - Permutations
    - And more
    
#### [{parsnip}](https://parsnip.tidymodels.org/)

- Provides a framework and flow for implementing a range of models
- Instead of remembering which model and functions come from which package and the unique properties of each, we can let {parsnip} do the complicated work and use the same format for any model deployment
- Creating the model object uses familiar Tidyverse syntax with the `%>%` pipe and a `fit()` function at the end

#### [{recipes}](https://recipes.tidymodels.org/)

- Allows us to create a series of steps defining how a model will be assembled
- Similar to cooking recipes that communicate quantities, ingredients, and sequence of events, we can establish a similar recipe any time we're ready to fit a model
- Thanks to the Tidyverse, syntax and setup is familiar with the `%>%` pipe
    
#### [{workflows}](https://workflows.tidymodels.org/)

- Recipes and models are technically separate, one eventually feeding the other
- A workflow can bundle them together so everything can stay in the same workspace and come together in a single `fit()` call
- It's more of less as easy as creatinga workflow and adding a 1) recipe and a 2) model
- From there we fit on the workflow

#### [{tune}](https://tune.tidymodels.org/)

- This packages allows us to tune our hyperparameters
- Hyperparameters are values that affect the learning but are defined as constants before training begins
    - Some basic examples are the train-test split ratio, learning rates, etc.
        
#### [{yardstick}](https://yardstick.tidymodels.org/)

- Used to evaluate a model's performance
- No one wants to calculate performance measures by hand so {yardstick} takes care of that and works with the other packages above
    
#### [{broom}](https://broom.tidymodels.org/)

- Gives us insight and summaries of our model
- Remember, models are objects with lots of components; {broom} helps us keep a pulse on what really matters from the model, such as:
    - Feature summaries
    - Feature and overall performance measures
    - Etc.

#### [{dials}](https://dials.tidymodels.org/)

- {tune} is for getting the hyperparameters optimized, while {dials} gives us an easy way to create and manage them


[{modeldata}](https://modeldata.tidymodels.org/) is an included collection of datasets optimized for use with {tidymodels}. From housing to diseases, museums to cricket chrips. There's surely a dataset or two to spike your interest in experimenting with these packages. I'm particularly interested in *Chicago ridership data*, *Sacramento CA home prices*, and *Sample time series drink data*.

There are other, supporting packages for use in data models which require individual library calls. These can be reviewed in depth on the [{tidymodels} Website](https://www.tidymodels.org/packages/#specialized-packages).

When installing and loading {tidymodels}, a couple additional packages are brought along for the ride. They have less to do with data models and more to do with general interaction with data. These are:

- [{dplyr}](): data transformation
- [{ggplot2}](): visualization 
- [{purrr}](): iteration
- [{tibble}](): simplified data frames
- [{tidyr}](): data frame cleanup
    
With that basic understanding of what we're dealing with, let's look at some code and examples from each package one by one.


### {rsample}

For exploring this package, we'll use the `Sacramento` dataset. We preview the data set below:

```{r}
sac_data <- modeldata::Sacramento
sac_data
```

#### The basics

This package is all about sampling. The documentation is a bit out of order from a sequence of events, but here's how I interpret it.

Often we want to chunk up our data set right off the bat into "training" and "testing". We do this with the `initial_split` function:

```{r}
set.seed(814)

split_obj <- initial_split(sac_data, prop = 0.8)

sac_train <- training(split_obj)
sac_test <- testing(split_obj)

nrow(sac_data) # Total observations
nrow(sac_train) # Training observations
nrow(sac_test) # Testing observations
```

There are some parameters to group splits by a certain variable but for the moment I'm not concerned with that angle.

Ultimately, with data science, it's not enough to train and/or test a single time. It's important to ensure statistically that the performance we're seeing isn't due to chance.

At this stage, we want to resample from a training or testing set. The documentation explains that an `rset` class is used to make a  collection of resamples. If we did a 5-fold cross-validation, we'd see 5 resamples in a collection of an `rset` class.

From the documentation, it appears we create these "resamples" using some of the cross-validation functions. We'll take a look with "bootstraps".

```{r}
bt_resamples <- bootstraps(sac_test, 5)
bt_resamples
```

We can see the five splits and their id. Each split has some figures listed. If we take a closer peek, there's a label for the list and the documentation explains further:

```{r}
bt_resamples$splits[[1]]
```

{rsample} clarifies that "Analysis" means those that were selected for the resample. Because we used the bootstrap method, all of the source data is included in the analysis.

The "Assessment" section is described as what's left out of the the analysis set. I'm unclear if that applies to this example or not.

From my vantage point, the terminology is weird but the key is that this exercise succeeds a train-test split.

We can use this split object to get the actual corresponding data for either the "Analysis" or "Assessment" groups.

```{r}
first_resample <- bt_resamples$splits[[1]]

# Gives us the "Analysis" set by default
head(as.data.frame(first_resample))

# We can specify the "Assessment" set
head(as.data.frame(first_resample, data = "assessment"))
```

Those seem a bit wordy so I'd much rather use their shortcut functions `analysis(first_resample)` or `assessment(firs_resample)`.

#### In practice

So using these functions in some type of data science project would go something like this in general:

1. Inspect the data and isolate a feature of interest
    1. i.e. we care about the `price` of a home
2. Explore the data for some ideal predictors or inputs of an eventual model
    1. i.e. the `sqft` and number of `beds` likely impact the sale price
3. Construct a model we want to evaluate
    1. i.e. `price ~ sqft + beds`
4. We now want to evaluate using some sampling methods

We'll start by picking a cross-validation method and setting up a model formula as described above. The documentation picks v-fold so we will too:

```{r}
v_samp <- vfold_cv(sac_data, v = 10, repeats = 10)
v_samp

model_form <- as.formula(price ~ sqft + beds)
```

We're going to have 100 splits total, 10 folds on the same iteration. We can now grab an "analysis" set, fit a model, predict the assessment data, and evaluate...for each resample split. The documentation demonstrates a function to do it. So I can practice and learn, I'll modify the function to our example.

```{r}
results <- function(splits, ...) {
  # We'll first fit a model to the analysis set
  model <- lm(..., data = analysis(splits))
  
  # We can combine predictions with the analysis set
  preds <- broom::augment(model, newdata = assessment(splits))
  
  # We'll return the features we care about
  preds %>% select(sqft, beds, price, .fitted)
}
```

If we now use the function over our splits, we'll have a comprehensive test. Now we gotta see if we can understand the output and such.

```{r}
# Using the very first split with the function
example <- results(v_samp$splits[[1]], model_form)

example
```

To iterate over the entire splits object of resamples, we can use the `purrr` package.

```{r}
v_samp$results <- map(v_samp$splits, results, model_form)

v_samp
```

Now we start to care about the performance of the model accross all these samples. Remember, all these samples are trying to simulate new data.

